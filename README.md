# ğŸ…ğŸ BaghChal AI with Reinforcement Learning

This project implements and compares various reinforcement learning strategies for the traditional Nepali board game **BaghChal** (Tiger and Goat). It features custom environments, training procedures, and evaluation for four RL algorithms: **Q-Learning, SARSA, Expected SARSA, and Deep Q-Network (DQN)**. You can also play against the trained agents via the terminal.

## ğŸ§  Algorithms Implemented

- ğŸ”µ Q-Learning
- ğŸŸ¡ SARSA
- ğŸŸ¢ Expected SARSA (eSARSA)
- ğŸ”´ DQN (Deep Q-Learning with PyTorch)

Each algorithm is trained in a self-play setup, and their performance is evaluated based on:
- ğŸ“ˆ Reward progression over time
- ğŸ¥‡ Win-rate distribution between tigers and goats
- ğŸ§­ Exploration decay and convergence trends

---

## ğŸ® Game: BaghChal (Tiger and Goat)

BaghChal is a two-player asymmetric strategy game:
- ğŸ… 4 Tigers try to **capture** goats by jumping over them.
- ğŸ 20 Goats try to **immobilize** all tigers by blocking their moves.

ğŸ“š Learn more: [Wikipedia - BaghChal](https://en.wikipedia.org/wiki/Bagh-Chal)

---

## ğŸ› ï¸ Project Structure

| File                        | Description                                                  |
|-----------------------------|--------------------------------------------------------------|
| `DQN.py`                   | Full implementation of environment, agents, training, UI     |
| `goat_dqn_model.pt`        | Trained goat DQN model (PyTorch)                             |
| `tiger_dqn_model.pt`       | Trained tiger DQN model (PyTorch)                            |
| `*_q_table.pkl`            | Saved Q-tables for SARSA, Q-Learning, Expected SARSA         |
| `README.md`                | You're reading it!                                           |

---

## ğŸš€ How to Run

### ğŸ”§ Install Requirements
```bash
pip install matplotlib torch numpy scikit-learn
```

### ğŸ‹ï¸ Train Agents (Optional)
Each algorithm has a training loop in `DQN.py`. Run it directly:
```bash
python DQN.py
```

Models and Q-tables are automatically saved after training.

---

## ğŸ“Š Visualizations

The script generates multiple useful plots:
- ğŸ“ˆ **Training Reward Trend**
- ğŸ“‰ **Epsilon Decay (Exploration vs Exploitation)**
- ğŸ§® **Win Distribution per Algorithm**
- ğŸ§  **Moving Average Reward**
- ğŸ” **Confusion Matrix (optional for DQN/SARSA)**

Example:
```
ğŸ Goat won 70% games in DQN
ğŸ… Tiger dominated in SARSA with 80% win rate
```

You can modify and save these plots inside the code using `matplotlib`.

---

## ğŸ“‹ Evaluation Summary

| Algorithm      | ğŸ… Tiger Win Rate | ğŸ Goat Win Rate | âš–ï¸ Notes                                         |
|----------------|-------------------|------------------|-------------------------------------------------|
| SARSA          | 80%               | 20%              | Best performing for tiger agents                |
| Q-Learning     | 60%               | 40%              | Decent baseline                                 |
| Expected SARSA | 70%               | 30%              | Slightly below SARSA                            |
| DQN            | 30%               | 70%              | Goats learned strong defense via function approx|

---

## ğŸ•¹ï¸ Human vs AI - Terminal Game

To play against trained agents:
```python
user_play()
```

- ğŸ‘¤ Choose role: `goat` or `tiger`
- ğŸ¯ Select actions from printed list
- ğŸ§© Game board rendered with emojis:
  - â¬œ Empty
  - ğŸ Goat
  - ğŸ… Tiger

---

## ğŸ§ª Research & Paper Context

This code was developed for a term research paper comparing RL algorithms for multi-agent systems. Inspired by:
- ğŸ¤– AlphaGoâ€™s success in Go
- ğŸ² Reinforcement learning in asymmetric environments
- ğŸ“š Prior work on Heterogeneous Q-Learning in BaghChal

---

## ğŸ‘¨â€ğŸ’» Authors

- ğŸ‘¨â€ğŸ“ Narayan Bhattarai  
- ğŸ‘¨â€ğŸ“ Deepak Pradhan  
ğŸ“§ Emails:  
`narayan.bhattarai@coyotes.usd.edu`  
`deepak.pradhan@coyotes.usd.edu`

---

## ğŸ“„ License

This repository is intended for academic and research purposes only.  
Please give credit if you use or extend this work. ğŸ™

---

## ğŸ™Œ Acknowledgments

- ğŸ›ï¸ University of South Dakota â€“ Dept. of Computer Science  
- ğŸ‘¨â€ğŸ« Prof. Dr. Srikant Baride (Supervision & Feedback)  
- ğŸ“– Sutton & Barto â€“ RL textbook  
- ğŸ™ OpenAI Gym & GitHub community inspirations  

---
